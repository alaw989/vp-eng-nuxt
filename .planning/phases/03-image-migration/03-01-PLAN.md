---
phase: 03-image-migration
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - .planning/scripts/download-images.ts
  - package.json
  - .planning/audit/raw-images.json
autonomous: true

must_haves:
  truths:
    - "All images from vp-associates.com WordPress Media API are downloaded"
    - "All images from HTML page crawling are downloaded"
    - "Download failures are retried 3 times with exponential backoff (1s, 2s, 4s)"
    - "Duplicate images are detected by SHA-256 hash and logged"
    - "Downloaded images are stored in .planning/audit/images/raw/ directory"
  artifacts:
    - path: ".planning/scripts/download-images.ts"
      provides: "Image download script with retry logic and deduplication"
      min_lines: 200
      exports: ["downloadAllImages", "fetchWordPressMedia", "crawlPagesForImages", "downloadImageWithRetry"]
    - path: ".planning/audit/raw-images.json"
      provides: "Catalog of all downloaded images with metadata"
      contains: "[{\"sourceUrl\",\"hash\",\"localPath\""
    - path: ".planning/audit/images/raw/"
      provides: "Directory containing all downloaded original images"
  key_links:
    - from: ".planning/scripts/download-images.ts"
      to: "https://vp-associates.com/wp-json/wp/v2/media"
      via: "fetchWordPressMedia function"
      pattern: "wp-json/wp/v2/media"
    - from: ".planning/scripts/download-images.ts"
      to: ".planning/audit/pages.json"
      via: "Import pages list for HTML crawling"
      pattern: "pages.json"
---

<objective>
Download all images from the source WordPress site (vp-associates.com) using both the WordPress Media API and HTML page crawling. Implement retry logic with exponential backoff for network resilience. Store downloaded images with SHA-256 hash-based deduplication tracking.

Purpose: Raw image downloads are the foundation for all subsequent optimization work. Without comprehensive image discovery and download, optimization cannot proceed.

Output: Download script (.planning/scripts/download-images.ts), raw images directory (.planning/audit/images/raw/), and image catalog (.planning/audit/raw-images.json)
</objective>

<execution_context>
@/home/deck/.claude/get-shit-done/workflows/execute-plan.md
@/home/deck/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-image-migration/03-CONTEXT.md
@.planning/phases/03-image-migration/03-RESEARCH.md
@.planning/audit/pages.json
</context>

<tasks>

<task type="auto">
  <name>Install Sharp dependency for image processing</name>
  <files>package.json</files>
  <action>
    Install Sharp package using npm:
    ```bash
    npm install sharp
    ```

    Do NOT install image processing alternatives (Jimp, ImageMagick, GM). Sharp is the standard stack decision from RESEARCH.md.

    Verify installation completes successfully. Sharp downloads prebuilt binaries for Linux x64.
  </action>
  <verify>`npm list sharp` returns version info without errors</verify>
  <done>Sharp v0.33.x installed and available for import</done>
</task>

<task type="auto">
  <name>Create image download script with retry logic and dual discovery</name>
  <files>.planning/scripts/download-images.ts</files>
  <action>
    Create TypeScript script at `.planning/scripts/download-images.ts` with the following structure:

    **Imports:**
    - `sharp` for image processing (metadata extraction)
    - `cheerio` for HTML parsing
    - `createHash` from `crypto` for SHA-256 hashing
    - `fs/promises` and `path` for file operations

    **Interfaces:**
    ```typescript
    interface RawImageEntry {
      sourceUrl: string;
      hash: string;
      localPath: string;
      method: 'media-api' | 'html-crawl';
      metadata: {
        width: number;
        height: number;
        format: string;
        sizeBytes: number;
      };
    }

    interface RawImagesCatalog {
      generated: string;
      sourceUrl: string;
      images: RawImageEntry[];
      summary: {
        totalImages: number;
        duplicatesFound: number;
        totalBytes: number;
        byMethod: {
          mediaApi: number;
          htmlCrawl: number;
        };
      };
    }
    ```

    **Functions to implement:**

    1. `downloadImageWithRetry(url: string, maxRetries = 3): Promise<Buffer>`
       - Exponential backoff: 1s, 2s, 4s delays between retries
       - Use native `fetch` API
       - Throw error after max retries exceeded

    2. `calculateHash(buffer: Buffer): string`
       - Use `createHash('sha256')` from Node.js crypto
       - Return hex digest

    3. `async fetchWordPressMedia(baseUrl: string): Promise<string[]>`
       - Paginate through `/wp-json/wp/v2/media?per_page=100&page=N`
       - Extract `source_url` from each media item
       - Stop on 400 status (page out of range)
       - Return array of image URLs

    4. `crawlPagesForImages(): Promise<string[]>`
       - Load pages list from `.planning/audit/pages.json`
       - For each page, fetch HTML and extract image URLs using Cheerio
       - Check for: `img[src]`, `img[data-src]`, `img[srcset]`, `picture source[srcset]`
       - Deduplicate URLs using Set
       - Return array of unique image URLs

    5. `async downloadAllImages(): Promise<RawImagesCatalog>`
       - Combine URLs from both WordPress Media API and HTML crawling
       - For each URL: download, calculate hash, check for duplicates
       - If new image: save to `.planning/audit/images/raw/{hash}.{ext}`
       - Track source URL, hash, local path, method, and metadata
       - Gather summary statistics

    **Main execution:**
    - Call `downloadAllImages()`
    - Write catalog to `.planning/audit/raw-images.json`
    - Log summary (total images, duplicates, total size)

    **Important:**
    - Create output directory if it doesn't exist
    - Use `sharp(buffer).metadata()` to extract width, height, format
    - Do NOT process/optimize images in this script (that's 03-02)
    - Store original files exactly as downloaded
    - Log progress for visibility (every 10 images)
  </action>
  <verify>`npx tsx .planning/scripts/download-images.ts` completes without errors and creates .planning/audit/raw-images.json</verify>
  <done>Script runs successfully, downloads images from both Media API and HTML crawling, outputs catalog file</done>
</task>

<task type="auto">
  <name>Execute download script and verify results</name>
  <files>.planning/audit/raw-images.json, .planning/audit/images/raw/</files>
  <action>
    Run the download script:
    ```bash
    npx tsx .planning/scripts/download-images.ts
    ```

    After completion, verify:
    1. `.planning/audit/raw-images.json` exists and contains valid JSON
    2. `.planning/audit/images/raw/` directory contains image files
    3. Catalog summary shows images discovered from both methods

    Log the summary statistics from the catalog output.
  </action>
  <verify>Check that raw-images.json contains at least 10 entries (actual site likely has many more)</verify>
  <done>All images downloaded and cataloged with metadata, ready for optimization phase</done>
</task>

</tasks>

<verification>
After completion, verify:
1. Sharp is installed: `npm list sharp`
2. Download script exists and is syntactically valid TypeScript
3. `.planning/audit/raw-images.json` exists with valid JSON structure
4. `.planning/audit/images/raw/` contains downloaded image files
5. Catalog shows discovery from both WordPress Media API and HTML crawling
6. SHA-256 hashes are present for all images
7. No duplicate hash values in the catalog (duplicates properly detected)

Run: `cat .planning/audit/raw-images.json | jq '.summary'` to view summary statistics
</verification>

<success_criteria>
1. Script `.planning/scripts/download-images.ts` exists and runs without errors
2. Sharp dependency installed in package.json
3. At least 10 images downloaded (actual count depends on source site)
4. Catalog file contains proper structure with sourceUrl, hash, localPath for each image
5. Images saved to `.planning/audit/images/raw/` directory
6. Summary section in catalog shows total images, duplicates found, and discovery method breakdown
</success_criteria>

<output>
After completion, create `.planning/phases/03-image-migration/03-01-SUMMARY.md` with:
- Total images discovered (by method)
- Duplicates detected
- Total raw file size
- Any network errors or retries encountered
- List of unique image formats found
</output>
