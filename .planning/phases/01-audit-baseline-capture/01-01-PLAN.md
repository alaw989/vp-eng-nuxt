---
phase: 01-audit-baseline-capture
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - .planning/scripts/enumerate-pages.ts
  - .planning/audit/pages.json
  - package.json
autonomous: true

must_haves:
  truths:
    - "All pages from vp-associates.com are discovered and enumerated"
    - "pages.json contains complete page inventory with URLs and metadata"
    - "Sitemap XML parsing successfully extracts all public content"
    - "REST API enrichment adds titles and content metadata"
    - "Slug normalization creates consistent identifiers"
  artifacts:
    - path: ".planning/scripts/enumerate-pages.ts"
      provides: "Page enumeration script with sitemap + REST API hybrid"
      min_lines: 80
      exports: ["main", "fetchSitemapUrls", "normalizeUrl"]
    - path: ".planning/audit/pages.json"
      provides: "Complete page inventory with metadata"
      contains: "[{\"url\":\"https://www.vp-associates.com/"
    - path: "package.json"
      provides: "Dependency declarations"
      contains: "\"cheerio\":"
  key_links:
    - from: ".planning/scripts/enumerate-pages.ts"
      to: "https://www.vp-associates.com/wp-sitemap.xml"
      via: "HTTP GET with xmlMode parsing"
      pattern: "fetch.*wp-sitemap\\.xml"
    - from: ".planning/scripts/enumerate-pages.ts"
      to: "https://www.vp-associates.com/wp-json/"
      via: "REST API calls for metadata enrichment"
      pattern: "fetch.*wp-json.*wp/v2"
    - from: ".planning/scripts/enumerate-pages.ts"
      to: ".planning/audit/pages.json"
      via: "fs/promises writeFile"
      pattern: "writeFile.*pages\\.json"
---

<objective>
Create a page enumeration script that discovers all pages from vp-associates.com WordPress installation and saves them to pages.json for use in baseline capture.

Purpose: REQ-AUD-001 requires a complete page inventory before any visual baselines can be captured. The sitemap.xml provides comprehensive coverage, while REST API enriches with metadata.

Output: Executable TypeScript script that generates `.planning/audit/pages.json` with all pages, URLs, slugs, and metadata.
</objective>

<execution_context>
@/home/deck/.claude/get-shit-done/workflows/execute-plan.md
@/home/deck/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/01-audit-baseline-capture/01-RESEARCH.md
@.planning/REQUIREMENTS.md

# Key implementation details from research:
- WordPress sitemap at: https://www.vp-associates.com/wp-sitemap.xml
- Sitemap uses index format with sub-sitemaps per content type
- REST API available at: https://www.vp-associates.com/wp-json/
- Hybrid approach: sitemap for complete URL list, REST API for metadata enrichment
- Cheerio for XML parsing (xmlMode: true)
- Use TypeScript interfaces for type safety
</context>

<tasks>

<task type="auto">
  <name>Install dependencies and create audit directory</name>
  <files>package.json</files>
  <action>
    1. Run `npm install --save-dev cheerio @types/cheerio` to add XML parsing capability
    2. Create `.planning/audit/` directory if it doesn't exist
    3. Create `.planning/scripts/` directory if it doesn't exist

    Do NOT modify any other dependencies. Only add cheerio for XML parsing.
  </action>
  <verify>
    1. `grep '"cheerio"' package.json` returns the dependency line
    2. `ls -d .planning/audit .planning/scripts` shows both directories exist
  </verify>
  <done>
    cheerio is installed in package.json and both required directories exist
  </done>
</task>

<task type="auto">
  <name>Create page enumeration script with sitemap parser</name>
  <files>.planning/scripts/enumerate-pages.ts</files>
  <action>
    Create `.planning/scripts/enumerate-pages.ts` with the following implementation:

    1. Import dependencies: `cheerio` for XML parsing, `fs/promises` for file operations

    2. Define TypeScript interfaces:
       ```typescript
       interface SitemapUrl {
         loc: string
         lastmod?: string
         changefreq?: string
         priority?: number
       }

       interface WpPage {
         id: number
         link: string
         slug: string
         title: { rendered: string }
         date: string
         modified: string
         status: string
       }

       interface PageEntry {
         url: string
         slug: string
         title?: string
         type: 'page' | 'service' | 'project' | 'post' | 'taxonomy' | 'archive'
         lastmod?: string
         source: 'sitemap' | 'api'
       }
       ```

    3. Implement `fetchSitemapUrls(sitemapUrl: string)` function:
       - Fetch the sitemap URL
       - Use Cheerio with `xmlMode: true` to parse XML
       - Check if it's a sitemap index (has `<sitemap>` elements)
       - If index: recursively fetch each sub-sitemap
       - If regular sitemap: extract all `<url>` elements with `<loc>`, `<lastmod>`, `<changefreq>`, `<priority>`
       - Return array of SitemapUrl objects

    4. Implement `normalizeUrl(url: string)` function:
       - Parse URL to extract pathname
       - Handle root path "/" as "home" slug
       - Single path segment (e.g., "/about") -> slug = segment, type = "page"
       - Two path segments (e.g., "/services/consulting") -> slug = second segment, type = first segment singularized (services -> service)
       - Return { slug: string, type: string }

    5. Implement `fetchWpPages()` function for metadata enrichment:
       - Fetch from `/wp/v2/pages?per_page=100&_fields=link,slug,title,date,modified,status`
       - Fetch from `/wp/v2/services?per_page=100&_fields=link,slug,title,date,modified,status`
       - Fetch from `/wp/v2/projects?per_page=100&_fields=link,slug,title,date,modified,status`
       - Return combined array of WpPage objects

    6. Implement `mergeWithApiData(sitemapPages: PageEntry[], apiPages: WpPage[])` function:
       - Create Map of API pages by URL for O(1) lookup
       - For each sitemap page, find matching API page
       - If found, add title from API data
       - Return enriched PageEntry array

    7. Implement `main()` async function:
       - Log "Enumerating pages from sitemap..."
       - Call `fetchSitemapUrls('https://www.vp-associates.com/wp-sitemap.xml')`
       - Log found URL count
       - Normalize URLs to extract slugs and types
       - Fetch API pages for enrichment
       - Merge sitemap data with API metadata
       - Create `.planning/audit/` directory if needed
       - Write to `.planning/audit/pages.json` with JSON.stringify(data, null, 2)
       - Log success message with page count

    8. Add error handling with try/catch and console.error
    9. Add `main().catch(console.error)` at EOF to execute

    Source base URL: https://www.vp-associates.com
    Sitemap URL: https://www.vp-associates.com/wp-sitemap.xml
    API base: https://www.vp-associates.com/wp-json/wp/v2/

    DO NOT use any other URL - these are the exact source site endpoints.
  </action>
  <verify>
    Run `npx tsx .planning/scripts/enumerate-pages.ts` and verify:
    1. Script executes without errors
    2. `.planning/audit/pages.json` is created
    3. File contains valid JSON array
    4. Array has at least 6 entries (home, about, contact, careers, portfolio, services)
    5. Each entry has url, slug, type, source fields
    6. Root URL has slug "home"
  </verify>
  <done>
    pages.json exists with complete page inventory from vp-associates.com, including URLs, slugs, types, and metadata
  </done>
</task>

</tasks>

<verification>
After all tasks complete, verify:
1. `cat .planning/audit/pages.json | jq '. | length'` returns count >= 6
2. `cat .planning/audit/pages.json | jq '.[0]'` shows proper structure with url, slug, type, source
3. All entries have valid URLs starting with https://www.vp-associates.com
4. At least one entry has type "service" or "project" confirming custom post type discovery
</verification>

<success_criteria>
1. pages.json contains all discoverable pages from vp-associates.com
2. Every entry has: url (string), slug (string, kebab-case), type (string enum), source ("sitemap" or "api")
3. Sitemap index recursively parsed to include all sub-sitemaps
4. REST API enrichment adds titles to matching pages
5. Script is re-runnable (idempotent) - produces same output on subsequent runs
</success_criteria>

<output>
After completion, create `.planning/phases/01-audit-baseline-capture/01-01-SUMMARY.md` with:
- Number of pages enumerated
- Breakdown by type (pages, services, projects, posts)
- Any anomalies or special cases handled
- Exact file path to generated pages.json
</output>
