---
phase: 05-qa-&-pwa-foundation
plan: 04
type: execute
wave: 3
depends_on: [05-03]
files_modified: [package.json, scripts/lighthouse-audit.js, scripts/pre-commit.js, .planning/audit/lighthouse.json]
autonomous: true

must_haves:
  truths:
    - "Lighthouse audit runs on build output with 85+ score targets"
    - "Lighthouse results saved to .planning/audit/lighthouse.json for trend tracking"
    - "Pre-commit hook includes Lighthouse audit"
    - "Build fails on low Lighthouse scores (below 85 threshold)"
  artifacts:
    - path: "package.json"
      provides: "lighthouse and chrome-launcher dependencies"
      contains: "lighthouse, chrome-launcher"
    - path: "scripts/lighthouse-audit.js"
      provides: "Lighthouse audit script with budget checking"
      contains: "runLighthouse, checkBudgets"
    - path: ".planning/audit/lighthouse.json"
      provides: "Lighthouse results for trend tracking"
    - path: "scripts/pre-commit.js"
      provides: "Updated pre-commit script with Lighthouse integration"
      contains: "lighthouse-audit"
  key_links:
    - from: "scripts/pre-commit.js"
      to: "scripts/lighthouse-audit.js"
      via: "dynamic import"
      pattern: "lighthouse-audit"
    - from: "scripts/lighthouse-audit.js"
      to: "chrome-launcher"
      via: "import"
      pattern: "chrome-launcher"
    - from: "scripts/lighthouse-audit.js"
      to: ".planning/audit/lighthouse.json"
      via: "fs.writeFileSync"
      pattern: "lighthouse.json"
---

<objective>
Implement Lighthouse performance benchmarking with 85+ score targets. Integrate Lighthouse audit into pre-commit workflow and save results for trend tracking.

Purpose: Per Phase 5 Context, Lighthouse score targets are 85+ (relaxed from roadmap's 90/95/100). Pre-commit hook should include full CI pipeline (build + preview + Lighthouse). Results stored in .planning/audit/lighthouse.json create performance history.

Output: Lighthouse and chrome-launcher installed, lighthouse-audit.js script created, pre-commit.js updated with Lighthouse integration, .planning/audit/lighthouse.json with baseline scores.
</objective>

<execution_context>
@/home/deck/.claude/get-shit-done/workflows/execute-plan.md
@/home/deck/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/phases/05-qa-&-pwa-foundation**---build-testing-infrastructure-and-pwa-features/05-CONTEXT.md
@.planning/phases/05-qa-&-pwa-foundation**---build-testing-infrastructure-and-pwa-features/05-RESEARCH.md
@.planning/phases/05-qa-&-pwa-foundation**---build-testing-infrastructure-and-pwa-features/05-03-SUMMARY.md
@package.json
@scripts/pre-commit.js
</context>

<tasks>

<task type="auto">
  <name>Install Lighthouse and chrome-launcher dependencies</name>
  <files>package.json</files>
  <action>
    Install Lighthouse and chrome-launcher for programmatic performance auditing:

    npm install -D lighthouse chrome-launcher

    These packages:
    - lighthouse: Google's official performance auditing library
    - chrome-launcher: Launches headless Chrome for Lighthouse runs

    Verify installation in package.json devDependencies.
  </action>
  <verify>
    - package.json contains "lighthouse" in devDependencies
    - package.json contains "chrome-launcher" in devDependencies
    - node_modules/lighthouse and node_modules/chrome-launcher exist
  </verify>
  <done>
    Lighthouse and chrome-launcher are installed as dev dependencies.
  </done>
</task>

<task type="auto">
  <name>Create Lighthouse audit script with budget checking</name>
  <files>scripts/lighthouse-audit.js</files>
  <action>
    Create scripts/lighthouse-audit.js with the following content:

    ```javascript
    #!/usr/bin/env node

    import fs from 'fs';
    import lighthouse from 'lighthouse';
    import * as chromeLauncher from 'chrome-launcher';
    import { fileURLToPath } from 'url';
    import { dirname, join } from 'path';

    const __filename = fileURLToPath(import.meta.url);
    const __dirname = dirname(__filename);

    // Performance budgets from Phase 5 Context: 85+ for all categories
    const BUDGETS = {
      performance: 85,
      accessibility: 85,
      seo: 85,
      'best-practices': 85
    };

    /**
     * Run Lighthouse audit on a URL
     * @param {string} url - URL to audit
     * @returns {Promise<LH.Result>} Lighthouse results
     */
    async function runLighthouse(url) {
      const chrome = await chromeLauncher.launch({
        chromeFlags: ['--headless', '--no-sandbox', '--disable-gpu', '--disable-dev-shm-usage']
      });

      try {
        const options = {
          logLevel: 'silent',
          output: 'json',
          onlyCategories: ['performance', 'accessibility', 'best-practices', 'seo'],
          port: chrome.port
        };

        const runnerResult = await lighthouse(url, options);
        const lhr = runnerResult.lhr;

        // Save JSON report for trend tracking
        const auditDir = join(__dirname, '../.planning/audit');
        if (!fs.existsSync(auditDir)) {
          fs.mkdirSync(auditDir, { recursive: true });
        }

        const reportPath = join(auditDir, 'lighthouse.json');

        // Read existing results to maintain history
        let history = [];
        if (fs.existsSync(reportPath)) {
          try {
            const existing = JSON.parse(fs.readFileSync(reportPath, 'utf-8'));
            history = existing.history || [];
          } catch (e) {
            // File corrupted, start fresh
            history = [];
          }
        }

        // Add new run to history
        const runData = {
          timestamp: new Date().toISOString(),
          scores: {
            performance: lhr.categories.performance.score * 100,
            accessibility: lhr.categories.accessibility.score * 100,
            'best-practices': lhr.categories['best-practices'].score * 100,
            seo: lhr.categories.seo.score * 100
          }
        };

        history.push(runData);

        // Keep last 30 runs
        if (history.length > 30) {
          history = history.slice(-30);
        }

        // Save with history
        fs.writeFileSync(reportPath, JSON.stringify({
          latest: runData,
          history,
          budgets: BUDGETS
        }, null, 2));

        return lhr;
      } finally {
        await chrome.kill();
      }
    }

    /**
     * Check Lighthouse results against budgets
     * @param {LH.Result} lhr - Lighthouse results
     * @returns {Array<{category: string, score: number, budget: number, diff: number}>}
     */
    function checkBudgets(lhr) {
      const failures = [];

      for (const [category, budget] of Object.entries(BUDGETS)) {
        const score = lhr.categories[category]?.score * 100 || 0;

        if (score < budget) {
          failures.push({
            category,
            score: Math.round(score),
            budget,
            diff: budget - score
          });
        }
      }

      return failures;
    }

    /**
     * Format scores for display
     * @param {LH.Result} lhr - Lighthouse results
     * @returns {string} Formatted score string
     */
    function formatScores(lhr) {
      const lines = [];
      for (const [key, category] of Object.entries(lhr.categories)) {
        if (BUDGETS[key]) {
          const score = Math.round(category.score * 100);
          const status = score >= BUDGETS[key] ? 'PASS' : 'FAIL';
          lines.push(`  ${key.padEnd(20)} ${score} ${status}`);
        }
      }
      return lines.join('\n');
    }

    // Run audit if called directly
    if (import.meta.url === `file://${process.argv[1]}`) {
      const url = process.argv[2] || 'http://localhost:3000';

      console.log(`Running Lighthouse audit on ${url}...\n`);

      runLighthouse(url)
        .then(lhr => {
          console.log('Lighthouse scores:');
          console.log(formatScores(lhr));
          console.log('');

          const failures = checkBudgets(lhr);

          if (failures.length > 0) {
            console.error('Budget failures:');
            failures.forEach(f => {
              console.error(`  ${f.category}: ${f.score} (below ${f.budget} by ${f.diff})`);
            });
            console.error('');
            console.error(`Results saved to .planning/audit/lighthouse.json`);
            process.exit(1);
          }

          console.log('All budgets met!');
          console.log(`Results saved to .planning/audit/lighthouse.json`);
          process.exit(0);
        })
        .catch(err => {
          console.error('Lighthouse failed:', err.message);
          process.exit(1);
        });
    }

    export { runLighthouse, checkBudgets, formatScores, BUDGETS };
    ```

    Make the script executable:
    chmod +x scripts/lighthouse-audit.js
  </action>
  <verify>
    - scripts/lighthouse-audit.js exists and is executable
    - Script defines BUDGETS with 85+ targets (per Phase 5 Context)
    - Script exports runLighthouse, checkBudgets, formatScores functions
    - Script saves results to .planning/audit/lighthouse.json with history
  </verify>
  <done>
    Lighthouse audit script created with 85+ budget targets and history tracking.
  </done>
</task>

<task type="auto">
  <name>Update pre-commit script to include Lighthouse audit</name>
  <files>scripts/pre-commit.js</files>
  <action>
    Update scripts/pre-commit.js to integrate Lighthouse audit:

    1. Read the existing pre-commit.js
    2. Add Lighthouse audit as Step 4 (before cleanup)
    3. Import and use runLighthouse and checkBudgets from lighthouse-audit.js

    Updated structure:
    - Step 1: Build
    - Step 2: Start preview server
    - Step 3: Check for hydration issues
    - Step 4: Run Lighthouse audit (NEW)
    - Step 5: Cleanup

    Add after hydration check and before cleanup:

    ```javascript
    // Step 4: Run Lighthouse audit
    console.log('\nStep 4/5: Running Lighthouse audit...');

    const { runLighthouse, checkBudgets, formatScores } = await import('./lighthouse-audit.js');

    const lhr = await runLighthouse(PREVIEW_URL);

    console.log('Lighthouse scores:');
    console.log(formatScores(lhr));

    const failures = checkBudgets(lhr);

    if (failures.length > 0) {
      console.error('\nLighthouse budget failures:');
      failures.forEach(f => {
        console.error(`  ${f.category}: ${f.score} (below ${f.budget} by ${f.diff})`);
      });

      throw new Error('Lighthouse scores below budget (85+). Improve performance or adjust budgets.');
    }

    console.log('Lighthouse scores passed!');
    ```

    Also update step numbers in console.log messages (5 steps total now).
  </action>
  <verify>
    - scripts/pre-commit.js imports from lighthouse-audit.js
    - Pre-commit Step 4 runs Lighthouse audit
    - Pre-commit fails on Lighthouse budget failures
    - Step 5 is cleanup (formerly Step 4)
  </verify>
  <done>
    Pre-commit script now includes Lighthouse audit with 85+ budget enforcement.
  </done>
</task>

<task type="auto">
  <name>Create .planning/audit directory and run baseline Lighthouse audit</name>
  <files>.planning/audit/lighthouse.json</files>
  <action>
    Ensure .planning/audit directory exists and run a baseline Lighthouse audit:

    1. Create audit directory if needed:
       mkdir -p .planning/audit

    2. Build the application:
       npm run build

    3. Start preview server in background:
       npm run preview &
       PREVIEW_PID=$!
       sleep 5

    4. Run Lighthouse audit:
       node scripts/lighthouse-audit.js http://localhost:3000

    5. Kill preview server:
       kill $PREVIEW_PID

    6. Verify .planning/audit/lighthouse.json was created with:
       - latest.scores (performance, accessibility, best-practices, seo)
       - history array with one entry
       - budgets object showing 85+ targets

    Note: If any scores are below 85, the script will exit with error 1. This is expected behavior - the audit is working. Note the failing categories for potential optimization.
  </action>
  <verify>
    - .planning/audit/ directory exists
    - .planning/audit/lighthouse.json exists
    - lighthouse.json contains latest.scores, history array, and budgets
    - All scores are recorded (even if below budget)
  </verify>
  <done>
    Baseline Lighthouse audit completed. Results saved to .planning/audit/lighthouse.json with history tracking.
  </done>
</task>

<task type="auto">
  <name>Test end-to-end pre-commit workflow with Lighthouse</name>
  <files>.husky/pre-commit, scripts/pre-commit.js, scripts/lighthouse-audit.js</files>
  <action>
    Test the complete pre-commit workflow:

    1. Create a minor change to trigger commit:
       echo "// Pre-commit test" > test-lighthouse-commit.ts

    2. Stage the file:
       git add test-lighthouse-commit.ts

    3. Attempt commit (runs full pipeline: build + preview + Lighthouse):
       git commit -m "test: verify lighthouse in pre-commit"

    Expected flow:
    - Build runs
    - Preview server starts
    - Hydration check passes
    - Lighthouse audit runs
    - Scores displayed
    - If all scores >= 85: commit succeeds
    - If any score < 85: commit fails with message

    4. If commit succeeded, clean up:
       git reset HEAD~1
       rm test-lighthouse-commit.ts

    5. If commit failed due to Lighthouse scores, that's expected behavior:
       - Note which categories failed
       - Clean up the staged file:
         git reset HEAD test-lighthouse-commit.ts
         rm test-lighthouse-commit.ts

    The important thing is that the pre-commit hook ran Lighthouse and enforced the budget.

    Optional: To test successful commit without Lighthouse gate, use:
    git commit --no-verify -m "test: bypass lighthouse"
    Then clean up.
  </action>
  <verify>
    - Pre-commit hook triggers Lighthouse audit
    - Lighthouse scores are displayed
    - Commit succeeds if all scores >= 85
    - Commit fails if any score < 85
    - Results saved to .planning/audit/lighthouse.json
  </verify>
  <done>
    End-to-end pre-commit workflow tested with Lighthouse integration.
  </done>
</task>

</tasks>

<verification>
Overall verification of Lighthouse performance benchmarking:

1. Lighthouse and chrome-launcher installed
2. scripts/lighthouse-audit.js runs audit and saves to .planning/audit/lighthouse.json
3. scripts/pre-commit.js integrates Lighthouse audit as Step 4
4. .planning/audit/lighthouse.json contains history of runs
5. Budget targets are 85+ (performance, accessibility, seo, best-practices)
6. Pre-commit fails on scores below 85
7. Bypass with --no-verify works for emergency commits

Run: npm run build && npm run preview &
Then: node scripts/lighthouse-audit.js http://localhost:3000
Finally: git commit to test full workflow
</verification>

<success_criteria>
1. Lighthouse audit script runs on build output
2. Score targets are 85+ (per Phase 5 Context, not 90/95/100 from roadmap)
3. Results saved to .planning/audit/lighthouse.json with history
4. Pre-commit hook includes Lighthouse audit
5. Build fails on low scores (< 85)
6. Lighthouse history tracks performance over time
</success_criteria>

<output>
After completion, create `.planning/phases/05-qa-&-pwa-foundation**---build-testing-infrastructure-and-pwa-features/05-04-SUMMARY.md` with:
- Lighthouse installation summary
- Baseline scores from first audit
- Pre-commit integration details
- .planning/audit/lighthouse.json structure description
- Any optimization recommendations if scores below 85
</output>
